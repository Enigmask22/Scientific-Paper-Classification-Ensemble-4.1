# ğŸ§  Scientific Paper Topic Classification - Ensemble Learning

<div align="center">

![Topic Classification](https://img.shields.io/badge/Topic_Classification-v1.0.0-blue?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)
![AI Engine](https://img.shields.io/badge/AI_Engine-Ensemble_Learning-red?style=for-the-badge)
![Accuracy](https://img.shields.io/badge/Best_Accuracy-87.5%25-green?style=for-the-badge)

**Scientific paper topic classification using Ensemble Learning techniques**

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“– Documentation](#-documentation) â€¢ [ğŸ› ï¸ Technologies](#ï¸-technologies) â€¢ [ğŸ“Š Results](#-results)

</div>

## ğŸŒŸ Key Features

### ğŸ¤– Ensemble Learning Pipeline
- **5 powerful algorithms** - Random Forest, AdaBoost, Gradient Boosting, XGBoost, LightGBM
- **3 vectorization methods** - Bag-of-Words, TF-IDF, Sentence Embeddings
- **High accuracy** - Up to 87.5% with XGBoost + Embeddings
- **Comprehensive comparison** - 15 combinations to find optimal approach

### ğŸ“Š Text Processing & Analysis
- **Smart preprocessing** - Remove noise, normalize text
- **Sentence Embeddings** - Using multilingual-e5-base model
- **Stratified sampling** - Ensure balanced data
- **Real-time evaluation** - Detailed metrics for each model

### ğŸ¯ Scientific Categories
- **5 main domains** - astro-ph, cond-mat, cs, math, physics
- **arXiv dataset** - 1000+ high-quality abstracts
- **Multi-class classification** - Accurate topic classification

## ğŸ› ï¸ CÃ´ng nghá»‡

### ğŸ Python Stack
![Python](https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-F7931E?style=flat&logo=scikit-learn&logoColor=white)
![XGBoost](https://img.shields.io/badge/XGBoost-FF6600?style=flat&logo=xgboost&logoColor=white)
![LightGBM](https://img.shields.io/badge/LightGBM-FFB000?style=flat&logo=lightgbm&logoColor=white)

- **Python 3.10+** - Core programming language
- **Scikit-learn** - Machine learning framework
- **XGBoost** - Gradient boosting framework
- **LightGBM** - Microsoft's gradient boosting
- **NumPy & Pandas** - Data manipulation
- **Matplotlib & Seaborn** - Data visualization

### ğŸ¤– AI & NLP
![Hugging Face](https://img.shields.io/badge/Hugging_Face-FF6B6B?style=flat&logo=huggingface&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=flat&logo=pytorch&logoColor=white)
![Sentence Transformers](https://img.shields.io/badge/Sentence_Transformers-FF6B6B?style=flat&logo=sentence-transformers&logoColor=white)

- **Hugging Face Datasets** - arXiv abstracts dataset
- **Sentence Transformers** - Multilingual embeddings
- **PyTorch** - Deep learning backend
- **Multilingual E5** - State-of-the-art embedding model

## ğŸ¯ Problem Statement

Given an arXiv abstract, predict its primary scientific category. The dataset includes abstracts labeled with categories like `astro-ph`, `cond-mat`, `cs`, `math`, and `physics`. The challenge is to evaluate which text representation and ensemble method produce the best classification performance.

## ğŸ”¬ Methodology

### ğŸ“Š Data Pipeline
1. **Data Loading**
   - Load dataset: `UniverseTBD/arxiv-abstracts-large` from Hugging Face Datasets
   - Cache directory: `./cache`

2. **Sampling Strategy**
   - Select the first 1,000 valid samples where the paper has exactly one category
   - Categories: `['astro-ph', 'cond-mat', 'cs', 'math', 'physics']`

3. **Text Preprocessing**
   - Strip and normalize whitespace
   - Remove line breaks, special characters, and digits
   - Lowercase text
   - Map string labels to integer IDs

4. **Train/Test Split**
   - Stratified split with 80% train and 20% test

### ğŸ¤– Model Architecture
5. **Text Vectorization**
   - **Bag-of-Words** with `CountVectorizer`
   - **TF-IDF** with `TfidfVectorizer`
   - **Sentence Embeddings** using `SentenceTransformer('intfloat/multilingual-e5-base')`
     - Queries formatted as `query: <text>` to leverage model's instruction-tuned format
     - Normalize embeddings

6. **Ensemble Learners**
   - **Random Forest** - Bootstrap aggregating
   - **AdaBoost** - Adaptive boosting
   - **Gradient Boosting** - sklearn implementation
   - **XGBoost** - Extreme gradient boosting
   - **LightGBM** - Microsoft's gradient boosting

7. **Evaluation Metrics**
   - Compute accuracy and classification report for each modelâ€“vectorizer pair
   - Compare across BoW, TF-IDF, and Embeddings

## ğŸ“Š Káº¿t quáº£

### ğŸ† Performance Comparison

| Model                | BoW (Accuracy) | TF-IDF (Accuracy) | Embeddings (Accuracy) |
|---------------------:|:--------------:|:-----------------:|:---------------------:|
| **Random Forest**    |     79.50%     |       77.50%      |        **85.50%**     |
| **AdaBoost**         |     65.50%     |       67.00%      |        **79.50%**     |
| **Gradient Boosting**|     **80.50%** |       79.50%      |        **86.50%**     |
| **XGBoost**          |     78.50%     |       76.50%      |        **87.50%**     |
| **LightGBM**         |     79.50%     |       **80.00%**  |        **87.50%**     |

### ğŸ¯ Key Insights
- **ğŸ† Best Performance**: XGBoost + Embeddings (87.5%)
- **ğŸ“ˆ Embeddings Advantage**: Consistently outperform BoW/TF-IDF
- **ğŸŒ³ Tree Models**: Gradient Boosting shows strong baseline performance
- **âš¡ LightGBM**: Fastest training with competitive accuracy

### ğŸ’¡ Optimization Tips
- **Embeddings**: Often help on semantic tasks, but tree models on dense embeddings may need parameter tuning
- **Feature Engineering**: Try limiting features or using `max_features` to control dimensionality
- **Hyperparameter Tuning**: Adjust `n_estimators`, `learning_rate` for better performance

## ğŸš€ Báº¯t Ä‘áº§u nhanh

### ğŸ“‹ YÃªu cáº§u há»‡ thá»‘ng
- **Python 3.10+** vá»›i pip
- **4GB+ RAM** (khuyáº¿n nghá»‹ 8GB+ cho embeddings)
- **Internet connection** (cho Hugging Face datasets)

### âš¡ CÃ i Ä‘áº·t tá»± Ä‘á»™ng

```bash
# 1. Clone repository
git clone <repository-url>
cd scientific-paper-classification

# 2. Táº¡o virtual environment
python -m venv venv

# Windows
venv\Scripts\activate
# macOS/Linux  
source venv/bin/activate

# 3. CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### ğŸ”§ Troubleshooting

<details>
<summary><strong>ğŸ Python Dependencies Issues</strong></summary>

```bash
# Náº¿u PyTorch/LightGBM build fails trÃªn Windows:
# CÃ i Ä‘áº·t Microsoft C++ Build Tools
# Hoáº·c sá»­ dá»¥ng pre-built wheels:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install lightgbm --only-binary=all
```
</details>

### ğŸƒâ€â™‚ï¸ Cháº¡y á»©ng dá»¥ng

```bash
# Cháº¡y pipeline hoÃ n chá»‰nh
python main.py
```

**QuÃ¡ trÃ¬nh thá»±c hiá»‡n:**
- ğŸ“¥ Downloads vÃ  cache Hugging Face dataset
- ğŸ”„ Samples vÃ  preprocesses 1,000 abstracts  
- ğŸ§® Builds BoW, TF-IDF, vÃ  embeddings
- ğŸ¤– Trains RandomForest, AdaBoost, GradientBoosting, XGBoost, LightGBM
- ğŸ“Š Prints accuracy cho má»—i configuration

### ğŸ›ï¸ Customization

```python
# Thay Ä‘á»•i categories trong main.py
CATEGORIES_TO_SELECT = ['astro-ph', 'cond-mat', 'cs', 'math', 'physics']

# Äiá»u chá»‰nh sample size
if len(samples) >= 1000:  # Thay Ä‘á»•i sá»‘ lÆ°á»£ng samples
    break

# Switch embedding model
embedding_vectorizer = EmbeddingVectorizer(model_name='intfloat/multilingual-e5-base')

# Tweak hyperparameters
def train_and_test_random_forest(..., n_estimators: int = 100):
```

### ğŸ“š Extended Analysis
- **Jupyter Notebook**: `Topic_Modeling_SHAP.ipynb` cho SHAP-based interpretation
- **Memory Requirements**: Embeddings computation cÃ³ thá»ƒ tá»‘n nhiá»u RAM
- **GPU Support**: CÃ³ thá»ƒ sá»­ dá»¥ng GPU cho sentence transformers

## ğŸ§ª Testing

```bash
# Test individual components
python -c "import main; print('All imports successful')"

# Test data loading
python -c "from datasets import load_dataset; print('Dataset loading OK')"

# Test model training
python -c "from sklearn.ensemble import RandomForestClassifier; print('Models OK')"
```

## ğŸ“¦ Project Structure

```
scientific-paper-classification/
â”œâ”€â”€ ğŸ“„ main.py                    # Main pipeline script
â”œâ”€â”€ ğŸ“‹ requirements.txt           # Python dependencies
â”œâ”€â”€ ğŸ““ Topic_Modeling_SHAP.ipynb  # Extended analysis notebook
â”œâ”€â”€ ğŸ“Š label_distribution.png     # Data visualization
â”œâ”€â”€ ğŸ“ˆ word_count_distribution.png # Text analysis charts
â”œâ”€â”€ ğŸ“ cache/                     # Dataset cache directory
â””â”€â”€ ğŸ“– README.md                  # Project documentation
```

## ğŸš€ Performance Benchmarks

### âš¡ Speed Metrics
- **Data Loading**: ~30s (first run), ~5s (cached)
- **Text Preprocessing**: ~10s for 1000 samples
- **BoW/TF-IDF**: ~5s vectorization
- **Embeddings**: ~60s (CPU), ~20s (GPU)
- **Model Training**: ~30s total for all models

### ğŸ’¾ Memory Usage
- **Dataset**: ~500MB cached
- **Embeddings**: ~200MB for 1000 samples
- **Models**: ~50MB total
- **Peak RAM**: ~2GB during embeddings

## ğŸ¤ ÄÃ³ng gÃ³p

ChÃºng tÃ´i hoan nghÃªnh má»i Ä‘Ã³ng gÃ³p! 

### ğŸ“ Development Workflow
1. Fork repository
2. Táº¡o feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push branch: `git push origin feature/amazing-feature`
5. Táº¡o Pull Request

### ğŸ¨ Code Standards
- **Python**: PEP 8 + Black formatter
- **Commits**: Conventional Commits
- **Documentation**: Docstrings cho má»i function
- **Testing**: Unit tests cho critical functions

## ğŸ“„ License

MIT License - xem [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t chi tiáº¿t.

## ğŸ™ Acknowledgments

- [Hugging Face](https://huggingface.co/) - Datasets vÃ  Transformers library
- [Scikit-learn](https://scikit-learn.org/) - Machine learning framework
- [XGBoost](https://xgboost.readthedocs.io/) - Gradient boosting framework
- [LightGBM](https://lightgbm.readthedocs.io/) - Microsoft's gradient boosting
- [Sentence Transformers](https://www.sbert.net/) - Semantic embeddings
- [arXiv](https://arxiv.org/) - Scientific paper dataset

---

<div align="center">

**ÄÆ°á»£c phÃ¡t triá»ƒn vá»›i â¤ï¸ bá»Ÿi AI Vietnam Team**

[![GitHub Stars](https://img.shields.io/github/stars/your-repo/scientific-paper-classification?style=social)](https://github.com/your-repo/scientific-paper-classification)
[![GitHub Forks](https://img.shields.io/github/forks/your-repo/scientific-paper-classification?style=social)](https://github.com/your-repo/scientific-paper-classification/fork)

[â­ Star repo nÃ y náº¿u nÃ³ há»¯u Ã­ch!](https://github.com/your-repo/scientific-paper-classification)

</div>